第一天
     深度学习--感知器  （神经元也叫感知器  y=f(w 。x +b) 拟合任何的线性函数  感知器规则调整权重和偏置）
     毕设4.1完成----4.1.2没有完成
     查找学习Tensorflow的资料
     DLinteview--机器学习相关基础
                 理解最小二乘法和最大似然估计：最小二乘法的核心是权衡，因为你要在很多条线中间选择，选择出距离所有的点之和最短的；
                                            而极大似然的核心是自恋，要相信自己是天选之子，自己看到的，就是冥冥之中最接近真相的。
                 PCA用来进行数据降维 方差最大化（数据越分散，某个特征效果越好） 协方差矩阵
                 XGBoost  Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。
                                         因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。
                 用到的树模型则是CART回归树模型  该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，
                                               每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。
    
    
    
    class Perceptron(object):
        def __init__(self, input_num, activator):
            '''
            初始化感知器，设置输入参数的个数，以及激活函数。
            激活函数的类型为double -> double
            '''
            self.activator = activator
            # 权重向量初始化为0
            self.weights = [0.0 for _ in range(input_num)]
            # 偏置项初始化为0
            self.bias = 0.0
        def __str__(self):
            '''
            打印学习到的权重、偏置项
            '''
            return 'weights\t:%s\nbias\t:%f\n' % (self.weights, self.bias)
        def predict(self, input_vec):
            '''
            输入向量，输出感知器的计算结果
            '''
            # 把input_vec[x1,x2,x3...]和weights[w1,w2,w3,...]打包在一起
            # 变成[(x1,w1),(x2,w2),(x3,w3),...]
            # 然后利用map函数计算[x1*w1, x2*w2, x3*w3]
            # 最后利用reduce求和
            return self.activator(
                reduce(lambda a, b: a + b,
                       map(lambda (x, w): x * w,  
                           zip(input_vec, self.weights))
                    , 0.0) + self.bias)
        def train(self, input_vecs, labels, iteration, rate):
            '''
            输入训练数据：一组向量、与每个向量对应的label；以及训练轮数、学习率
            '''
            for i in range(iteration):
                self._one_iteration(input_vecs, labels, rate)
        def _one_iteration(self, input_vecs, labels, rate):
            '''
            一次迭代，把所有的训练数据过一遍
            '''
            # 把输入和输出打包在一起，成为样本的列表[(input_vec, label), ...]
            # 而每个训练样本是(input_vec, label)
            samples = zip(input_vecs, labels)
            # 对每个样本，按照感知器规则更新权重
            for (input_vec, label) in samples:
                # 计算感知器在当前权重下的输出
                output = self.predict(input_vec)
                # 更新权重
                self._update_weights(input_vec, output, label, rate)
        def _update_weights(self, input_vec, output, label, rate):
            '''
            按照感知器规则更新权重
            '''
            # 把input_vec[x1,x2,x3,...]和weights[w1,w2,w3,...]打包在一起
            # 变成[(x1,w1),(x2,w2),(x3,w3),...]
            # 然后利用感知器规则更新权重
            delta = label - output
            self.weights = map(
                lambda (x, w): w + rate * delta * x,
                zip(input_vec, self.weights))
            # 更新bias
            self.bias += rate * delta
